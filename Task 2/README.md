
# Данные
Данные получены в формате '.jpeg' в двух папках, уже размеченные по классам.
Для создания датасета идеально подходит класс dataset.ImageFolder, который создает
датасет и маркирует классы на основе директории папок. В папке Female файлы получают лейбл [0],
в папке Male - [1].
Используются базовые трансформации изображений - приведение всех изображений к единому
размеру 112х112, и нормализация изображений.
Данные разбиты рандомно 'utils.data.random_split' на тренировочный и валидационный сеты на 80 и 20 тысяч
изображений соответственно.

# Параметры

Размер батча - 16
Скорость обучения - 0.001
Количество эпох  - 10

# Модель
Архитектура модели следующая:

  - Convolution Neural Network:

    1. Первый слой свертки с входящим тензором [16 x 3 x 112 x 112].
       Размерность входящего тензора [размер батча, каналы, высота, ширина]
       Размерность фильтра [3 х 3 х 8]
       Шаг фильтра stride [1 х 1]
       Padding [1 x 1]
       Параметры padding, stride выбраны так, чтобы сохранить высоту и ширину изображения.
       Выходящий тензор [16 x 8 x 112 x 112]

    2. Функция активации RELU
    3. Операция субдискретизации MaxPooling с размерностью [2 x 2]

    4. Второй слой свертки с входящим тензором [16 x 8 x 56 x 56].
       Размерность входящего тензора [размер батча, каналы, высота, ширина]
       Размерность фильтра [3 х 3 х 8]
       Шаг фильтра stride [1 х 1]
       Padding [1 x 1]
       Выходящий тензор [16 x 16 x 56 x 56]

    5. Функция активации RELU
    6. Операция субдискретизации MaxPooling с размерностью [2 x 2]
    7. Полносвязный линейный слой с входом[16 x 28 x 28] и выходом 2 (количество классов)

    Инициализация весов - Kaiming.
    Оптимизация - Adam.
    Функция потерь - Перекрестная Кросс-Энтропия.
    Это стандартный набор для классификации.

# Обучение

Обучение делаю с размером батча 16 и на 10 эпохах, размер изображения 112. Данные параметры выбраны, так как
в виду невозможности расчета на GPU, обучение происходило на процессоре и заняло всю ночь, а Google Colab отказывался принимать сто тысяч изображений.

Скорость обучения обновляется с использованием scheduler, когда в течение 5 эпох лосс не уменьшается, скорость обучения уменьшается на фактор 0.1.


# Результат
На 10 эпохах достигнута точность в 93.7% на валидационном сете.
